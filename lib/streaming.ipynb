{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"HealthCarePrediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel,GBTClassificationModel,DecisionTreeClassificationModel\n",
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"stroke\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "# rfModel = RandomForestClassificationModel.load('model/random_forest')\n",
    "# gbtModel = GBTClassificationModel.load('model/gbt')\n",
    "# dtModel = DecisionTreeClassificationModel.load('model/decision_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_to_scale(df, lower_skew=-2, upper_skew=2, dtypes='double'):\n",
    "    \n",
    "    selected_features = []\n",
    "\n",
    "    feature_list = list(df.toPandas().select_dtypes(include=[dtypes]).columns)\n",
    "\n",
    "    for feature in feature_list:\n",
    "\n",
    "        if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            \n",
    "            selected_features.append(feature)\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF = spark.read.csv('healthcare-dataset-stroke-data.csv', header=True, inferSchema=True)\n",
    "rawDF = rawDF.dropna()\n",
    "rawDF = rawDF.filter(rawDF['bmi'] != \"N/A\")\n",
    "rawDF = rawDF.filter(rawDF['gender'] != \"Other\")\n",
    "rawDF = rawDF.withColumn(\"bmi\",rawDF.bmi.cast('double'))\n",
    "cat_features = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "stringIndexedDF = rawDF\n",
    "for features in cat_features:\n",
    "    # Index Categorical Features\n",
    "    string_indexer = StringIndexer(inputCol=features, outputCol=features + \"_index\")\n",
    "    stringIndexedDF = string_indexer.fit(stringIndexedDF).transform(stringIndexedDF)\n",
    "for features in cat_features:     \n",
    "    stringIndexedDF = stringIndexedDF.withColumn(features+\"_index\",stringIndexedDF[features+\"_index\"].cast('int'))\n",
    "\n",
    "stringIndexedDF = stringIndexedDF.drop(*cat_features)\n",
    "\n",
    "X = stringIndexedDF.drop('stroke')\n",
    "Y = stringIndexedDF.select('stroke')\n",
    "stk = SMOTE(random_state=42)\n",
    "X_res,y_res = stk.fit_resample(X.toPandas(),Y.toPandas())\n",
    "joinDF = pd.concat([X_res, y_res], axis=1, join=\"inner\")\n",
    "stringIndexedDF = spark.createDataFrame(joinDF)\n",
    "\n",
    "stages = []\n",
    "num_features = ['age','avg_glucose_level', 'bmi', 'hypertension', 'heart_disease']\n",
    "index_features = ['gender_index', 'ever_married_index', 'work_type_index', 'Residence_type_index', 'smoking_status_index']\n",
    "for features in index_features:\n",
    "    encoder = OneHotEncoder(inputCols=[features],\n",
    "                                    outputCols=[features + \"_class_vec\"])\n",
    "    stages += [encoder]\n",
    "\n",
    "unscaled_features = select_features_to_scale(df=stringIndexedDF, lower_skew=-2, upper_skew=2, dtypes='double')\n",
    "unscaled_assembler = VectorAssembler(inputCols=unscaled_features, outputCol=\"unscaled_features\")\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"scaled_features\")\n",
    "stages += [unscaled_assembler, scaler]\n",
    "num_unscaled_diff_list = list(set(num_features) - set(unscaled_features))\n",
    "assembler_inputs = [feature + \"_class_vec\" for feature in index_features] + num_unscaled_diff_list\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"assembled_inputs\") \n",
    "stages += [assembler]\n",
    "assembler_final = VectorAssembler(inputCols=[\"scaled_features\",\"assembled_inputs\"], outputCol=\"features\")\n",
    "stages += [assembler_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline_model = pipeline.fit(stringIndexedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.157764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"stroke\", featuresCol=\"features\", maxIter=10)\n",
    "df_transform_fin = pipeline_model.transform(stringIndexedDF)\n",
    "train_data, test_data = df_transform_fin.randomSplit([.7, .3])\n",
    "gbtModel = gbt.fit(train_data)\n",
    "gbtPredictions = gbtModel.transform(test_data)\n",
    "accuracy = evaluator.evaluate(gbtPredictions)\n",
    "# gbtModel.save('model/gbt')\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start your server at this point\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from functools import partial\n",
    "\n",
    "inputStream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load() \\\n",
    "\n",
    "fields = partial(\n",
    "    regexp_extract, str=\"value\", pattern=\"^(\\w*)\\s*,\\s*(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)\"\n",
    ")\n",
    "\n",
    "topic = inputStream.select(\n",
    "    fields(idx=1).alias(\"id\"),\n",
    "    fields(idx=2).cast('long').alias(\"gender_index\"), \n",
    "    fields(idx=3).cast('double').alias(\"age\"), \n",
    "    fields(idx=4).cast('long').alias(\"hypertension\"),\n",
    "    fields(idx=5).cast('long').alias(\"heart_disease\"),\n",
    "    fields(idx=6).cast('long').alias(\"ever_married_index\"),\n",
    "    fields(idx=7).cast('long').alias(\"work_type_index\"),\n",
    "    fields(idx=8).cast('long').alias(\"Residence_type_index\"),\n",
    "    fields(idx=9).cast('double').alias(\"avg_glucose_level\"),\n",
    "    fields(idx=10).cast('double').alias(\"bmi\"),\n",
    "    fields(idx=11).cast('long').alias(\"smoking_status_index\")\n",
    ")\n",
    "\n",
    "query = topic \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"data_stream\")\\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "s = socket.socket()\n",
    "s.connect((\"localhost\",9999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while 1:\n",
    "    data = s.recv(1024)\n",
    "    row = data.decode().split(',')\n",
    "    sql = spark.sql(f\"SELECT * FROM {query.name}\")   \n",
    "    sql_row = sql.filter(sql.id == row[0])\n",
    "    while (sql_row.count() == 0):\n",
    "        time.sleep(5)\n",
    "        sql = spark.sql(f\"SELECT * FROM {query.name}\") \n",
    "        sql_row = sql.filter(sql.id == row[0])\n",
    "    row_transform = pipeline_model.transform(sql_row)\n",
    "    predict = gbtModel.transform(row_transform)\n",
    "    x = predict.select('id','prediction').rdd.collect()\n",
    "    predictMess = \"result:\"+\";\".join([\",\".join(map(str, item)) for item in x])\n",
    "    s.send(predictMess.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64778,0.0\n"
     ]
    }
   ],
   "source": [
    "# sql = spark.sql(f\"SELECT * FROM {query.name}\")   \n",
    "# sql_row = sql.filter(sql.id == 64778)\n",
    "# row_transform = pipeline_model.transform(sql_row)\n",
    "# # row_transform.select('gender_index_class_vec','ever_married_index_class_vec','work_type_index_class_vec','Residence_type_index_class_vec','smoking_status_index_class_vec','unscaled_features','scaled_features','assembled_inputs').filter(row_transform.id == 64778).show(1, False)\n",
    "# predict = gbtModel.transform(row_transform)\n",
    "# # predict.select('features','rawPrediction','probability','prediction').show(1, False)\n",
    "# x = predict.select('id','prediction').rdd.collect()\n",
    "# predictMess = \";\".join([\",\".join(map(str, item)) for item in x])\n",
    "# print(predictMess)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df989ace8a8da28cb7f7d1a12e3b4afec8c680846e24e9f4f7f420bf9f1fadcc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
