{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline\n",
    "from __future__ import print_function\n",
    "import seaborn as sns\n",
    "import imblearn\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import math \n",
    "import pyspark.sql.functions as F\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate();\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"HealthCarePrediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF = spark.read.csv('healthcare-dataset-stroke-data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF = rawDF.dropna()\n",
    "rawDF = rawDF.filter(rawDF['bmi'] != \"N/A\")\n",
    "rawDF = rawDF.filter(rawDF['gender'] != \"Other\")\n",
    "rawDF = rawDF.withColumn(\"bmi\",rawDF.bmi.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler, MinMaxScaler\n",
    "cat_features = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "stringIndexedDF = rawDF\n",
    "for features in cat_features:\n",
    "    # Index Categorical Features\n",
    "    string_indexer = StringIndexer(inputCol=features, outputCol=features + \"_index\")\n",
    "    stringIndexedDF = string_indexer.fit(stringIndexedDF).transform(stringIndexedDF)\n",
    "for features in cat_features:     \n",
    "    stringIndexedDF = stringIndexedDF.withColumn(features+\"_index\",stringIndexedDF[features+\"_index\"].cast('int'))\n",
    "\n",
    "stringIndexedDF = stringIndexedDF.drop(*cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------+-------------+-----------------+----+------+------------+------------------+---------------+--------------------+--------------------+\n",
      "|   id| age|hypertension|heart_disease|avg_glucose_level| bmi|stroke|gender_index|ever_married_index|work_type_index|Residence_type_index|smoking_status_index|\n",
      "+-----+----+------------+-------------+-----------------+----+------+------------+------------------+---------------+--------------------+--------------------+\n",
      "|64778|82.0|           0|            1|            208.3|32.5|     1|           1|                 0|              0|                   1|                   1|\n",
      "+-----+----+------------+-------------+-----------------+----+------+------------+------------------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringIndexedDF.filter(stringIndexedDF.id == 64778).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stringIndexedDF.drop('stroke')\n",
    "Y = stringIndexedDF.select('stroke')\n",
    "stk = SMOTE(random_state=42)\n",
    "X_res,y_res = stk.fit_resample(X.toPandas(),Y.toPandas())\n",
    "joinDF = pd.concat([X_res, y_res], axis=1, join=\"inner\")\n",
    "balancedData = spark.createDataFrame(joinDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_to_scale(df=balancedData, lower_skew=-2, upper_skew=2, dtypes='double'):\n",
    "    \n",
    "    # Empty Selected Feature List for Output\n",
    "    selected_features = []\n",
    "    \n",
    "    # Select Features to Scale based on Inputs ('in32' type, drop 'ID' columns or others, skew bounds)\n",
    "    feature_list = list(df.toPandas().select_dtypes(include=[dtypes]).columns)\n",
    "    \n",
    "    # Loop through 'feature_list' to select features based on Kurtosis / Skew\n",
    "    for feature in feature_list:\n",
    "\n",
    "        if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            \n",
    "            selected_features.append(feature)\n",
    "    \n",
    "    # Return feature list to scale\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features = ['gender_index', 'ever_married_index', 'work_type_index', 'Residence_type_index', 'smoking_status_index']\n",
    "\n",
    "encoderDF = balancedData\n",
    "\n",
    "for features in index_features:\n",
    "    encoder = OneHotEncoder(inputCols=[features],\n",
    "                                    outputCols=[features + \"_class_vec\"])\n",
    "    encoderDF = encoder.fit(encoderDF).transform(encoderDF)\n",
    "\n",
    "# encoderDF = encoderDF.drop(*index_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'stroke'\n",
    "stages = []\n",
    "num_features = ['age','hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']\n",
    "label_str_index =  StringIndexer(inputCol=label, outputCol=\"label_index\")\n",
    "\n",
    "# Scale Feature: Select the Features to Scale using helper 'select_features_to_scale' function above and Standardize \n",
    "unscaled_features = select_features_to_scale(df=encoderDF, lower_skew=-2, upper_skew=2, dtypes='double')\n",
    "\n",
    "unscaled_assembler = VectorAssembler(inputCols=unscaled_features, outputCol=\"unscaled_features\")\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "stages += [unscaled_assembler, scaler]\n",
    "\n",
    "# Create list of Numeric Features that Are Not Being Scaled\n",
    "num_unscaled_diff_list = list(set(num_features) - set(unscaled_features))\n",
    "\n",
    "# Assemble or Concat the Categorical Features and Numeric Features\n",
    "assembler_inputs = [feature + \"_class_vec\" for feature in index_features] + num_unscaled_diff_list\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"assembled_inputs\") \n",
    "\n",
    "stages += [label_str_index, assembler]\n",
    "\n",
    "# Assemble Final Training Data of Scaled, Numeric, and Categorical Engineered Features\n",
    "assembler_final = VectorAssembler(inputCols=[\"scaled_features\",\"assembled_inputs\"], outputCol=\"features\")\n",
    "\n",
    "stages += [assembler_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(encoderDF)\n",
    "\n",
    "df_transform = pipeline_model.transform(encoderDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------------+-------------------------+------------------------------+------------------------------+-----------------+-------------------+--------------------------------------------------+\n",
      "|gender_index_class_vec|ever_married_index_class_vec|work_type_index_class_vec|Residence_type_index_class_vec|smoking_status_index_class_vec|unscaled_features|scaled_features    |assembled_inputs                                  |\n",
      "+----------------------+----------------------------+-------------------------+------------------------------+------------------------------+-----------------+-------------------+--------------------------------------------------+\n",
      "|(1,[],[])             |(1,[0],[1.0])               |(4,[0],[1.0])            |(1,[],[])                     |(3,[1],[1.0])                 |[32.5]           |[4.826355838293409]|(14,[1,2,8,10,11,12],[1.0,1.0,1.0,82.0,208.3,1.0])|\n",
      "+----------------------+----------------------------+-------------------------+------------------------------+------------------------------+-----------------+-------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transform.select('gender_index_class_vec','ever_married_index_class_vec','work_type_index_class_vec','Residence_type_index_class_vec','smoking_status_index_class_vec','unscaled_features','scaled_features','assembled_inputs').filter(df_transform.id == 64778).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_transform.select('features').show(truncate=False)\n",
    "from pyspark.ml.classification import RandomForestClassificationModel,GBTClassificationModel,DecisionTreeClassificationModel\n",
    "\n",
    "# rfModel = RandomForestClassificationModel.load('model/random_forest')\n",
    "gbtLoadedModel = GBTClassificationModel.load('model/gbt')\n",
    "# dtModel = DecisionTreeClassificationModel.load('model/decision_tree')\n",
    "\n",
    "predict = gbtLoadedModel.transform(df_transform)\n",
    "# gbtModel.transform(df_transform)\n",
    "# dtModel.transform(df_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+----------------------------------------+----------------------------------------+----------+\n",
      "|features                                                              |rawPrediction                           |probability                             |prediction|\n",
      "+----------------------------------------------------------------------+----------------------------------------+----------------------------------------+----------+\n",
      "|(15,[0,2,3,9,11,12,13],[4.826355838293409,1.0,1.0,1.0,82.0,208.3,1.0])|[-0.5633275137631617,0.5633275137631617]|[0.24477893187401564,0.7552210681259843]|1.0       |\n",
      "+----------------------------------------------------------------------+----------------------------------------+----------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict.filter(predict.id == 64778).select('id','gender_index','age','hypertension','heart_disease','ever_married_index','work_type_index','Residence_type_index','avg_glucose_level','bmi','smoking_status_index').show()\n",
    "# predict.filter(predict.id == 64778).select('prediction').show()\n",
    "predict.filter(predict.id == 64778).select('features','rawPrediction','probability','prediction').show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64778,1.0\n"
     ]
    }
   ],
   "source": [
    "sql_row = encoderDF.filter(predict.id == 64778)\n",
    "row_transform = pipeline_model.transform(sql_row)\n",
    "predict = gbtModel.transform(row_transform)\n",
    "x = predict.select('id','prediction').rdd.collect()\n",
    "predictMess = \";\".join([\",\".join(map(str, item)) for item in x])\n",
    "print(predictMess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So lan xuat hien cua stroke la 0:  4699\n",
      "So lan xuat hien cua stroke la 1:  4699\n"
     ]
    }
   ],
   "source": [
    "df_transform_fin = df_transform.select('features','label_index')\n",
    "# df_transform_fin.show()\n",
    "# df_transform_fin.count()\n",
    "print (\"So lan xuat hien cua stroke la 0: \",df_transform_fin.filter(df_transform_fin['label_index'] == 0).count())\n",
    "print (\"So lan xuat hien cua stroke la 1: \",df_transform_fin.filter(df_transform_fin['label_index'] == 1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8169515669515669\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"label_index\", featuresCol=\"features\")\n",
    "train_data, test_data = df_transform_fin.randomSplit([.7, .3])\n",
    "test_data.drop(\"label_index\")\n",
    "model = dt.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "# model.save('model/decision_tree')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805401405845357\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label_index\", featuresCol=\"features\", numTrees=10)\n",
    "train_data, test_data = df_transform_fin.randomSplit([.7, .3])\n",
    "rfModel = rf.fit(train_data)\n",
    "predictions = rfModel.transform(test_data)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "# rfModel.save('model/random_forest')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.15229\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"label_index\", featuresCol=\"features\", maxIter=10)\n",
    "train_data, test_data = df_transform_fin.randomSplit([.7, .3])\n",
    "gbtModel = gbt.fit(train_data)\n",
    "gbtPredictions = gbtModel.transform(test_data)\n",
    "accuracy = evaluator.evaluate(gbtPredictions)\n",
    "gbtModel.save('model/gbt')\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df989ace8a8da28cb7f7d1a12e3b4afec8c680846e24e9f4f7f420bf9f1fadcc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
